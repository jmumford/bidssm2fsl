{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeanettemumford/miniconda3/envs/fitlins/lib/python3.9/site-packages/nilearn/datasets/__init__.py:93: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "from fitlins.interfaces.bids import ModelSpecLoader, LoadBIDSModel, BIDSSelect, BIDSDataSink\n",
    "\n",
    "from fitlins.interfaces.nistats import DesignMatrix, FirstLevelModel, SecondLevelModel\n",
    "\n",
    "from fitlins.interfaces.visualizations import (\n",
    "        DesignPlot, DesignCorrelationPlot, ContrastMatrixPlot, GlassBrainPlot)\n",
    "from fitlins.interfaces.utils import MergeAll, CollateWithMetadata\n",
    "\n",
    "from bids.layout import BIDSLayout\n",
    "from fitlins.utils.bids import load_all_specs\n",
    "\n",
    "from bids.tests import get_test_data_path\n",
    "from os.path import join\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeanettemumford/miniconda3/envs/fitlins/lib/python3.9/site-packages/bids/layout/validation.py:151: UserWarning: The PipelineDescription field was superseded by GeneratedBy in BIDS 1.4.0. You can use ``pybids upgrade`` to update your derivative dataset.\n",
      "  warnings.warn(\"The PipelineDescription field was superseded \"\n"
     ]
    }
   ],
   "source": [
    "from bids.modeling import BIDSStatsModelsGraph\n",
    "database_path = '/Users/jeanettemumford/Dropbox/Research/Projects/Fsf_converter/Data/ds003/dbcache'\n",
    "# This will need adjustment if dbcache doesn't already exist\n",
    "layout = BIDSLayout(\n",
    "    database_path=database_path\n",
    ")\n",
    "\n",
    "model_path = '/Users/jeanettemumford/Dropbox/Research/Projects/Fsf_converter/Data/model-ds0003_smdl.json'\n",
    "graph = BIDSStatsModelsGraph(layout, model_path)    \n",
    "\n",
    "kwargs = {'desc': 'preproc', 'space': 'MNI152NLin2009cAsym', 'subject': ['01', '02', '03']} \n",
    "graph.load_collections(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bids.modeling import transformations as tm\n",
    "\n",
    "# remove the convolve transform and all that are past it\n",
    "# This only happens to run or subject nodes (Look up all node name possibilities)\n",
    "sub_node = graph.get_node('subject')\n",
    "sub_trans = sub_node.transformations #has transformer and instructions\n",
    "sub_pybids_transforms = sub_trans['transformer'] #just the version of pybids transforms\n",
    "apply_trans = tm.TransformerManager(sub_pybids_transforms)\n",
    "collection_apply_to = sub_node._collections[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:  make this a function\n",
    "# Adjust the instructions\n",
    "sub_trans_instructions = sub_trans['instructions']\n",
    "\n",
    "transformation_names = [l['name'] for l in sub_trans_instructions]\n",
    "if 'Convolve' in transformation_names:\n",
    "    convolve_idx = transformation_names.index('Convolve')\n",
    "    trimmed_instructions = sub_trans_instructions[:convolve_idx]\n",
    "    convolve_inputs = sub_trans_instructions[convolve_idx]['input']\n",
    "else:\n",
    "    trimmed_instructions = sub_trans_instructions\n",
    "    convolve_inputs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_out = apply_trans.transform(collection_apply_to.clone(), trimmed_instructions)\n",
    "\n",
    "#This should work, since I don't want the sparse stuff\n",
    "\n",
    "#next step:\n",
    "#I think I can use sub_node.model to create the \"confound\" file and \"ev\" files\n",
    "# will also need to use sub_node.contrasts\n",
    "# find all variables involved in contrasts first\n",
    "# EV files will be the union of the contrast names and convolve_inputs\n",
    "# I will need to track which EVs require convolution and which do not (for fsf)\n",
    "# Maybe just the file name would be name_3column.txt and name_1column.txt\n",
    "# Then set up contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later (figuring out how database_path is dealth with)\n",
    "# From run.py for fitlins (so opts is the parsed args)\n",
    "    if opts.database_path is None:\n",
    "        database_path = Path(work_dir) / 'dbcache'\n",
    "        reset_database = True\n",
    "    else:\n",
    "        database_path = opts.database_path\n",
    "        reset_database = False\n",
    "#Generally I will need to copy code from run.py for  lots of things here's derivatives\n",
    "\n",
    "    derivatives = True if not opts.derivatives else opts.derivatives\n",
    "    # Need this when specifying args directly (i.e. neuroscout)\n",
    "    # god bless neuroscout, but let's make it work for others!\n",
    "    if isinstance(derivatives, list) and len(derivatives) == 1:\n",
    "        # WRONG AND EVIL to those who have spaces in their paths... bad bad practice\n",
    "        # TODO - fix neuroscout\n",
    "        derivatives = derivatives[0].split(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_db_path_make_layout(database_path):\n",
    "    try:\n",
    "        layout = BIDSLayout(database_path = database_path)\n",
    "    except:\n",
    "        layout = BIDSLayout(\n",
    "            root = root_dir,\n",
    "            database_path = database_path,\n",
    "#Maybe use this (from fitlins)\n",
    "    if opts.database_path is None:\n",
    "        database_path = Path(work_dir) / 'dbcache'\n",
    "        reset_database = True\n",
    "    else:\n",
    "        database_path = opts.database_path\n",
    "        reset_database = False\n",
    "    #I think the first entry is actually root = opts.bids_dir\n",
    "    layout = bids.BIDSLayout(\n",
    "        opts.bids_dir,\n",
    "        derivatives=derivatives,\n",
    "        database_path=database_path,\n",
    "        reset_database=reset_database\n",
    "    )\n",
    "\n",
    "\n",
    "        )\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "database_path_copy = '/Users/jeanettemumford/Dropbox/Research/Projects/Fsf_converter/Data_copy/ds003/dbcache'\n",
    "\n",
    "layout = BIDSLayout(root='/Users/jeanettemumford/Dropbox/Research/Projects/Fsf_converter/Data_copy/ds003',\n",
    "    derivatives='/Users/jeanettemumford/Dropbox/Research/Projects/Fsf_converter/Data_copy/ds003_fmriprep',\n",
    "    database_path=database_path_copy,\n",
    "    reset_database=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start turning the above into a function or two\n",
    "database_path = '/Users/jeanettemumford/Dropbox/Research/Projects/Fsf_converter/Data_copy/ds003/dbcache'\n",
    "# root is only needed if dbcache hasn't been made already\n",
    "root='/Users/jeanettemumford/Dropbox/Research/Projects/Fsf_converter/Data_copy/ds003'\n",
    "desc = 'preproc'\n",
    "space = 'MNI152NLin2009cAsym'\n",
    "subject = ['01', '02', '03']\n",
    "\n",
    "def func1(database_path, desc, space, subject):\n",
    "    layout = BIDSLayout(\n",
    "        database_path=database_path\n",
    "    )\n",
    "\n",
    "model_path = '/Users/jeanettemumford/Dropbox/Research/Projects/Fsf_converter/Data/model-ds0003_smdl.json'\n",
    "graph = BIDSStatsModelsGraph(layout, model_path)\n",
    "\n",
    "kwargs = {'desc': 'preproc', 'space': 'MNI152NLin2009cAsym', 'subject': ['01', '02', '03']} \n",
    "graph.load_collections(**kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to grab data path\n",
    "#'desc': 'preproc', 'space': 'MNI152NLin2009cAsym',\n",
    "\n",
    "junk1 = BIDSSelect(\n",
    "            database_path=database_path,\n",
    "            entities = {'subject': '01'},\n",
    "            selectors={\n",
    "                'suffix': 'bold',\n",
    "                'desc': 'preproc',\n",
    "                'space': 'MNI152NLin2009cAsym',\n",
    "                'extension': ['.nii', '.nii.gz', '.dtseries.nii', '.func.gii'],\n",
    "            },\n",
    "        )\n",
    "junk1.run()\n",
    "print(junk1._results.get('bold_files'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I should check how they deal with duplicates. or is it impossible?\n",
    "\n",
    "layout = BIDSLayout.load(database_path=database_path)\n",
    "filters = {\n",
    "    'suffix': ['T1w', 'bold'],\n",
    "    'subject': '01',\n",
    "    'desc': 'preproc',\n",
    "    'space': 'MNI152NLin2009cAsym',\n",
    "    'extension': ['.nii', '.nii.gz', '.dtseries.nii', '.func.gii']\n",
    "}\n",
    "output = layout.get(\n",
    "    **filters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = trans_out.entities['RepetitionTime']\n",
    "# User will specify output_dir\n",
    "output_dir = '/Users/jeanettemumford/Dropbox/Research/Projects/Fsf_converter/Data/ds003/bids_sm_to_fsf'\n",
    "feat_out_root=f'{output_dir}/feat_analyses'\n",
    "\n",
    "#Debating putting all additions in a dictionary\n",
    "\n",
    "fsf_settings = {}\n",
    "fsf_settings['set fmri(level)'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy stub and open up for additions!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll need this later.  The function to replace reg directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_reg_dir(featdir, t1_standard):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import shutil\n",
    "    if not os.path.isdir(featdir):\n",
    "        print('Input featdir directory does not exist')\n",
    "        return\n",
    "    if not os.path.exists(f'{featdir}/example_func.nii.gz'):\n",
    "        print('example_func.nii.gz not found \\n')\n",
    "        print('Check that input is feat directory')\n",
    "        return\n",
    "    shutil.rmtree(f'{featdir}/reg')\n",
    "    os.makedirs(f'{featdir}/reg')\n",
    "    ident_mat = np.identity(4)\n",
    "    mat_names = [\n",
    "        'example_func2highres', 'example_func2standard', 'highres2example_func',\n",
    "        'highres2standard', 'standard2example_func', 'standard2highres'\n",
    "    ]\n",
    "    for mat_name in mat_names:\n",
    "        np.savetxt(f'{featdir}/reg/{mat_name}.mat', ident_mat, delimiter=\" \")\n",
    "    shutil.copy(\n",
    "        f'{featdir}/example_func.nii.gz', f'{featdir}/reg/standard.nii.gz'\n",
    "    )\n",
    "    shutil.copy(\n",
    "        t1_standard, f'{featdir}/reg/highres.nii.gz'\n",
    "    )\n",
    "\n",
    "\n",
    "featdir = '/Users/jeanettemumford/Dropbox/Research/Talks/MumfordBrainStats/ds008_R1.1.0_raw/sub002/model/test_withstats3.feat'\n",
    "# Technically this is usually a T1 in subject space, but the standard space is what I think we'll want\n",
    "# This is the bg_image in higher levels (averaged)\n",
    "t1_standard = '/Users/jeanettemumford/Dropbox/Research/Talks/MumfordBrainStats/ds008_R1.1.0_raw/sub002/model/task001_run002.feat/reg/highres.nii.gz'\n",
    "replace_reg_dir(featdir, t1_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I might need this type of code to apply the filters\n",
    "# From statsmodels.py node.run() code line 437,  Loop up inputs and collections\n",
    "        # Filter inputs and collections if needed\n",
    "collection = node._collections[0]\n",
    "\n",
    "        if filters:\n",
    "            inputs = [i for i in inputs if matches_entities(i, filters)]\n",
    "            collections = [c for c in collections if matches_entities(c, filters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(graph, node=None, inputs=None, **filters):\n",
    "    if node is None:\n",
    "        node = graph.root_node\n",
    "\n",
    "    specs = node.run(inputs, group_by=node.group_by, **filters)\n",
    "    outputs = list(chain(*[s.contrasts for s in specs]))\n",
    "\n",
    "    base_entities = graph.model[\"input\"]\n",
    "\n",
    "    all_specs = {\n",
    "        node.name: [\n",
    "            {\n",
    "                'contrasts': [c._asdict() for c in spec.contrasts],\n",
    "                'entities': {**base_entities, **spec.entities},\n",
    "                'level': spec.node.level,\n",
    "                'X': spec.X,\n",
    "                'name': spec.node.name,\n",
    "                'model': spec.node.model,\n",
    "                # Metadata is only used in higher level models; save space\n",
    "                'metadata': spec.metadata if spec.node.level != \"run\" else None,\n",
    "            }\n",
    "            for spec in specs\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    for child in node.children:\n",
    "        all_specs.update(\n",
    "            load_graph(graph, child.destination, outputs, **child.filter)\n",
    "        )\n",
    "\n",
    "    return all_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouble shooting load_graph problem\n",
    "root = '/Users/jeanettemumford/Dropbox/Research/Projects/Fsf_converter/ModelZoo/my_models/ds000005/'\n",
    "database_path = '/Users/jeanettemumford/Dropbox/Research/Projects/Fsf_converter/ModelZoo/my_models/ds000005/dbcache'\n",
    "reset_database = True\n",
    "model_path = '/Users/jeanettemumford/Dropbox/Research/Projects/Fsf_converter/ModelZoo/my_models/test.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = BIDSLayout(\n",
    "        root = root,\n",
    "        database_path=database_path,\n",
    "        reset_database=reset_database\n",
    ")\n",
    "\n",
    "graph = BIDSStatsModelsGraph(layout, model_path)\n",
    "graph.load_collections(scan_length=480)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_specs = load_graph(graph, node=None, inputs=None)\n",
    "\n",
    "node = graph.root_node\n",
    "inputs = None\n",
    "specs = node.run(inputs, group_by=node.group_by)\n",
    "outputs = list(chain(*[s.contrasts for s in specs]))\n",
    "\n",
    "base_entities = graph.model[\"input\"]\n",
    "\n",
    "all_specs = {\n",
    "    node.name: [\n",
    "        {\n",
    "            'contrasts': [c._asdict() for c in spec.contrasts],\n",
    "            'entities': {**base_entities, **spec.entities},\n",
    "            'level': spec.node.level,\n",
    "            'X': spec.X,\n",
    "            'name': spec.node.name,\n",
    "            'model': spec.node.model,\n",
    "            # Metadata is only used in higher level models; save space\n",
    "            'metadata': spec.metadata if spec.node.level != \"run\" else None,\n",
    "        }\n",
    "        for spec in specs\n",
    "    ]\n",
    "}\n",
    "\n",
    "#Now for run level (see if it works)\n",
    "#for child in node.children:\n",
    "#    all_specs.update(\n",
    "#        load_graph(graph, child.destination, outputs, **child.filter)\n",
    "#    )\n",
    "\n",
    "node2 = node.children[0].destination\n",
    "inputs2 = outputs\n",
    "filter = node.children[0].filter\n",
    "\n",
    "#here's the error...\n",
    "specs2 = node2.run(inputs2, group_by=node2.group_by)\n",
    "#outputs = list(chain(*[s.contrasts for s in specs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(a=10, b=20, **kwargs):\n",
    "    print(a*b)\n",
    "    print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing a nilearn thing quickly\n",
    "\n",
    "from nilearn.glm.first_level.hemodynamic_models import _resample_regressor"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f183487e75c86b25d211dece0ae7dc68729df7aaa20f2c11ebe325676df970a7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fitlins')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
